# Unified Configuration for Anime Image Tagger
# This consolidates all previously scattered configs into the Configuration_System.py structure
# Generated from: augmentation, dataloader, dataset_prep, export, inference, logging, paths, runtime, training, validation, vocabulary configs

# Global Settings
project_name: "anime-image-tagger"
experiment_name: "default_experiment"
output_root: "./experiments"
log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
# Set a target for total system RAM usage. This is a guideline and not strictly enf
max_memory_gb: 120.0 # Adjusted for 128GB system (leaves 13GB headroom)
max_gpu_memory_gb: null

# --------------------------------------------------------------------
# Back-compat: migrated from legacy configs/paths.yaml
# These allow older scripts to read from a single unified place.
vocab_path: "./vocabulary.json"
log_dir: "./logs"
default_output_dir: "./outputs"

# Model Configuration
model:
    architecture_type: "vit_wide_shallow"
    hidden_size: 1152
    num_hidden_layers: 16
    num_attention_heads: 18
    intermediate_size: 4608
    image_size: 512
    patch_size: 16
    num_channels: 3
    use_cls_token: true
    use_style_token: true
    use_line_token: true
    use_color_token: true
    num_special_tokens: 4
    hidden_dropout_prob: 0.18
    attention_probs_dropout_prob: 0.18
    drop_path_rate: 0.15   # recommended range 0.08–0.3 (must be < 1.0)
    initializer_range: 0.02
    layer_norm_eps: 1.0e-6
    use_flex_attention: true  # Enables Flex Attention (PyTorch 2.5+)
    attention_bias: true
    flex_block_size: 128  # Block size for Flex Attention sparse computation
    token_ignore_threshold: 0.9
    num_labels: 0  # Auto-detect from vocabulary
    num_groups: 20
    tags_per_group: 10000
    gradient_checkpointing: false
    # checkpoint_every_n_layers: 4   # Checkpoint every 4th layer (layers 0,4,8,12 of 16)

# Data Configuration (consolidates dataloader.yaml, train_config.yaml, augmentation.yaml)
data:
  # Storage locations
  # Add the path to your raw training data (images and JSONs) here.
  # The script will look for .json files in this directory and then find the
  # corresponding images based on the "filename" field in each JSON.
  # WARNING: Update this path! Currently set to Linux path but running on Windows.
  # Example Windows path: "Z:/path/to/your/dataset/" or "C:/Users/YourName/datasets/"
  storage_locations:
    - path: "L:/Dab/Dab"
      priority: 0
      type: "local"
      enabled: true
    - path: "/home/user/datasets/anime_curated"
      priority: 1
      type: "local"
      enabled: false  # Disabled by default
    - path: "/mnt/das/anime_archive"
      priority: 2
      type: "das"
      enabled: false  # Disabled by default
  
  # Paths (from paths.yaml)
  hdf5_dir: "./teacher_features"
  vocab_dir: "./vocabulary"
  output_dir: "./outputs"
  
  # Image processing
  image_size: 512
  normalize_mean: [0.5, 0.5, 0.5]  # Anime-optimized (from inference_config.yaml)
  normalize_std: [0.5, 0.5, 0.5]   # Anime-optimized
  pad_color: [114, 114, 114]

  # Data loading (from dataloader.yaml)
  batch_size: 32
  num_workers: 10  # Arrow mmap shares metadata (~500MB/worker overhead), 32-core system
  pin_memory: true   # Enables async DMA transfer for PCIe 4.0/5.0
  prefetch_factor:  4 
  persistent_workers: true  # NOTE: auto-forced to false when num_workers == 0
  drop_last: false
  worker_log_level: WARNING  # Worker log level: WARNING allows debugging, CRITICAL minimizes overhead

  # Validation split limiting
  # Limits validation set size at split time (before Arrow cache loading).
  # Set to null to use the full 5% validation split (~276k for 5.5M dataset).
  max_val_samples: 30000

  preload_files: 0

  # Sidecar Tensor Cache (replaces L2 LMDB cache)
  # Preprocessed images are stored as .safetensor files alongside original images.
  # No virtual memory issues, pooled SSD-friendly, unlimited parallel reads.
  # NOTE: Provides no benefit if the system has CPU to spare - only helps when
  # CPU preprocessing (decode/resize) is the bottleneck starving the GPU.
  sidecar_cache_enabled: false
  sidecar_extension: ".safetensor"  # File extension for cached tensors
  sidecar_storage_dtype: "bfloat16"  # Storage dtype for cached tensors
  cpu_bf16_cache_pipeline: true  # Use bfloat16 in preprocessing pipeline if CPU supports it

  # Metadata Cache (JSON parsing cache)
  # Caches parsed JSON metadata to avoid re-parsing millions of files on every run.
  # First run: ~3-5 minutes to build cache with parallel processing
  # Subsequent runs: ~2-5 seconds to load from cache (940× speedup)
  # Cache size: ~1.8 GB for 5.6M images
  metadata_cache_enabled: true
  metadata_cache_workers: 20  # Number of parallel workers for cache creation
  force_rebuild_metadata_cache: false  # Set true to force cache rebuild
  metadata_cache_staleness_check_samples: 100  # Base sample size (scaled dynamically by dataset size)

  # Cache Validation Settings (v2.0)
  # New in v2.0: Improved cache validation and staleness detection
  split_cache_version: "2.0"  # Don't change unless format changes
  metadata_cache_version: "2.0"  # Don't change unless format changes
  metadata_cache_use_dynamic_sampling: true  # Scale sample size with dataset (100→10K for 5.6M files)
  metadata_cache_use_stratified_sampling: true  # Sample across directories for better coverage
  cache_count_tolerance_percent: 0.1  # 0.1% tolerance (down from 1%, stricter validation)
  cache_count_tolerance_min: 100  # Minimum absolute tolerance

  # Augmentation (from augmentation.yaml)
  # NOTE: Most augmentations are NOT IMPLEMENTED yet - only random_flip_prob is active.
  # With 5-6 million images, augmentation is generally unnecessary.
  augmentation_enabled: true
  random_flip_prob: 0.5  # IMPLEMENTED: Horizontal flipping with orientation tag swapping

  # NOT IMPLEMENTED: Color augmentations below are configured but not applied in training
  color_jitter: false
  color_jitter_brightness: 0.1
  color_jitter_contrast: 0.1
  color_jitter_saturation: 0.1
  color_jitter_hue: 0.05
  eye_color_weight_boost: 0.0

  # NOT IMPLEMENTED: Geometric augmentations below are configured but not applied in training
  random_crop_scale: [0.8, 1.0]
  random_rotation_degrees: 0.0

  # NOT IMPLEMENTED: Advanced augmentations below are configured but not applied in training
  randaugment_num_ops: 2
  randaugment_magnitude: 9
  mixup_alpha: 0.0
  cutmix_alpha: 0.0
  random_erasing_p: 0.0

  # Orientation mapping (from augmentation.yaml)
  orientation_map_path: "./configs/orientation_map.json"
  strict_orientation_validation: true
  skip_unmapped: true
  # Orientation flip safety policy.
  # Legacy mapping: strict → conservative, balanced → balanced, lenient → permissive
  orientation_safety_mode: "conservative"

# Training Configuration (from training_config.yaml)
training:
    memory_format: "channels_last"
    num_epochs: 25
    learning_rate: 1.0e-4
    weight_decay: 0.03
    gradient_accumulation_steps: 4  # Effective batch size: 32 × 4 = 128

    # Optimizer
    # NOTE: Using "adamw8bit" for memory-efficient 8-bit quantized optimizer states
    # The auto-config system (optimizer_config.py + scheduler_config.py) will automatically:
    #   - Scale learning rate based on effective batch size
    #   - Adjust weight decay based on dataset size
    #   - Select optimal scheduler (cosine/cosine_restarts)
    #   - Save memory with 8-bit quantized optimizer states (~3 GB savings vs standard adamw)
    optimizer: "adamw8bit"
    adam_beta1: 0.9
    adam_beta2: 0.999
    adan_beta3: 0.99
    adam_epsilon: 1.0e-8

    # Scheduler
    scheduler: "cosine_restarts"
    warmup_steps: 100  # number of OPTIMIZER STEPS to warm up (step-based)
    warmup_ratio: 0.0
    num_cycles: 3
    lr_end: 1.0e-6

    # Mixed precision
    # Enabled by default to reduce VRAM usage and improve performance.
    # bfloat16 is preferred for its numerical stability and range, similar to float32.
    use_amp: true
    amp_dtype: bfloat16           # one of: bfloat16, float16
    enable_anomaly_detection: false  # set true for debugging only

    # Gradient clipping
    gradient_clipping:
      enabled: true
      max_norm: 1.0

    # Optionally reduce LR after a non-finite loss to help recovery.
    overflow_backoff_on_nan:
      enabled: true
      factor: 0.1   # multiply each param group lr by this factor on NaN/Inf loss

    # Checkpointing
    save_steps: 10000
    save_total_limit: 3
    save_best_only: false
    # Resume behavior (choose per run/goal):
    #   "none" | "latest" | "best" | "/path/to/checkpoint.pt"
    # Tip: use "latest" while iterating, "best" for polishing/finetune,
    # or point to an exact checkpoint to branch experiments safely.
    resume_from: "latest"

    eval_steps: 15000  # ~3 validations per epoch (46,667 steps/epoch)
    logging_steps: 5000  # Reduced TensorBoard overhead (was 3000)
    # Resume-aware reseeding: avoid replaying the same early-epoch order after resume
    resume_reseed: true

    # Loss configuration
    tag_loss:
      alpha: 0.5            # Balancing factor between positive/negative examples
      gamma_neg: 1.0        # Focusing parameter for negatives
      gamma_pos: 1.0        # Focusing parameter for positives
      label_smoothing: 0.1  # Smoothing for tag targets (regularization boost)
      clip: 0.05            # Prevent extreme logits
    rating_loss:
      alpha: 0.5
      gamma_neg: 1.0
      gamma_pos: 1.0
      label_smoothing: 0.1  # Smoothing for rating targets
      clip: 0.05
    use_class_weights: true
  
    # Curriculum learning
    use_curriculum: true
    start_region_training_epoch: 20
    region_training_interval: 5
    curriculum_difficulty_schedule: "linear"

    # Hardware
    device: "cuda"
    distributed: false
    local_rank: -1
    world_size: 1
    ddp_backend: "nccl"

    # torch.compile() Optimization (PyTorch 2.0+)
    # Provides 15-35% speedup through graph optimization and kernel fusion
    # Especially effective for transformer models with attention mechanisms
    use_compile: true
    compile_mode: "max-autotune-no-cudagraphs"  # Triton autotuning without CUDA graph VRAM overhead
    compile_fullgraph: false  # Allow graph breaks for dynamic shapes (padding masks)
    compile_dynamic: true  # Enabled - prevents recompilation when batch size varies (e.g., corrupt images)
    # First compilation takes 2-5 minutes but is negligible over 60 epochs

    # Tracking
    use_tensorboard: true
    use_wandb: false
    wandb_project: "anime-tagger"
    wandb_run_name: null
    wandb_entity: null

    # Training stability (from runtime.yaml)
    seed: 42
    deterministic: false
    benchmark: true

    # Early stopping
    early_stopping_patience: 6
    early_stopping_threshold: 0.0001
    # Ignore first N epochs for early-stopping to avoid initial instability
    early_stopping_burn_in_epochs: 2
    # One of: median, mean, last, max
    early_stopping_burn_in_strategy: median

# Inference Configuration (from inference_config.yaml)
inference:
  model_path: "./checkpoints/best_model.pt"
  precision: "bf16"  # Set precision to bf16 for inference
  compile_model: false
  
  # Prediction
  prediction_threshold: 0.5
  adaptive_threshold: true
  min_predictions: 5
  max_predictions: 50
  top_k: null
  
  # Tag filtering
  filter_nsfw: false
  nsfw_tags:
    - "explicit"
    - "questionable"
    - "sensitive"
    - "nude"
    - "sexual_content"
  blacklist_tags: []
  whitelist_tags: null
  
  # Post-processing
  apply_implications: true
  resolve_aliases: true
  group_by_category: false
  remove_underscores: true
  
  # Performance
  use_tensorrt: false
  optimize_for_speed: false
  batch_timeout_ms: 100
  max_batch_size: 32
  
  # Output
  output_format: "json"
  include_scores: true
  score_decimal_places: 3
  sort_by_score: true
  
  # API Settings
  enable_api: false
  api_host: "0.0.0.0"
  api_port: 8000
  api_workers: 4
  api_max_image_size: 10485760
  api_rate_limit: 100
  api_cors_origins:
    - "*"
  
  # Caching
  enable_cache: true
  cache_ttl_seconds: 3600
  max_cache_size: 1000

# Export Configuration (from export_config.yaml)
export:
  export_format: "onnx"
  opset_version: 19
  export_params: true
  do_constant_folding: true
  dynamic_batch_size: true
  min_batch_size: 1
  max_batch_size: 128
  optimize: true
  optimize_for_mobile: false
  quantize: false
  quantization_type: "dynamic"
  calibration_dataset_size: 100
  validate_export: true
  tolerance_rtol: 1.0e-3
  tolerance_atol: 1.0e-5
  num_validation_samples: 10
  add_metadata: true
  model_description: "Anime Image Tagger Model"
  model_author: "AnimeTaggers"
  model_version: "1.0.0"
  model_license: "MIT"
  output_path: "./exported_model"

# ──────────────────────────────────────────────────────────────────────────────
# Validation (migrated from configs/validation_config.yaml)
# Centralized knobs for validation dataloader & preprocessing.
# These are read by validation_loop.py at runtime.
validation:
  max_samples: 30000  # Subsample validation set (30K of 280K) for faster epochs
  dataloader:
    batch_size: 128  # Balanced for validation throughput
    num_workers: 6  # Increased from 4 for smoother validation throughput
    pin_memory: true  # Enables async DMA transfer for faster CPU-GPU transfer
    prefetch_factor: 4   # Match training - deeper queue for consistent throughput
    persistent_workers: true  # NOTE: auto-forced to false when num_workers == 0
  preprocessing:
    # Uses data.normalize_mean/std by default; keep in sync with
    # training/inference unless intentionally different.
    image_size: 512
    patch_size: 16

# ──────────────────────────────────────────────────────────────────────────────
# Monitoring & Alerts
# Configures the monitoring system, including alerts for training events.
monitor:
  log_level: "INFO"
  log_to_file: true
  log_to_console: true

  # System resource tracking
  track_system_metrics: true
  system_metrics_interval: 30.0  # Background collection interval (seconds)
  system_metrics_log_interval_steps: 3000  # How often to log to TensorBoard (steps)
  track_gpu_metrics: true
  track_disk_io: true
  track_network_io: false

  # Visualization
  # Note: These are often overridden by the training config for convenience
  use_tensorboard: true
  # Histogram logging defaults (set conservative intervals; disable by default)
  log_histograms: false
  log_param_histograms: false
  log_grad_histograms: false
  histogram_log_interval: 4000
  param_hist_interval_steps: 4000
  grad_hist_interval_steps: 4000
  tensorboard_dir: "./tensorboard"
  use_wandb: false
  wandb_project: "anime-tagger"
  wandb_entity: null
  wandb_run_name: null

  # Alerts
  enable_alerts: true
  alert_on_gpu_memory_threshold: 0.9
  alert_on_cpu_memory_threshold: 0.9
  alert_on_disk_space_threshold: 0.95
  alert_on_training_stuck_minutes: 30
  alert_on_loss_explosion: 10.0
  alert_on_nan_loss: true
  # --------------------------------------------------------------------
  # --- WEBHOOK REVIEW POINT ---
  # The actual webhook URL is stored separately in sensitive_config.py.
  # Replace ALERT_WEBHOOK_URL in that file with your private Discord webhook.
  # alert_webhook_url: "your_webhook_url_here"
  # --------------------------------------------------------------------

  tb_image_logging:
    max_samples: 50         # Number of images to grade per logging interval
    topk: 50                # how many tags per sample in Text tab
    log_native_resolution: true  # keep images crisp in TB
    dpi_for_figures: 220          # only used if you later add composite figures
    image_log_steps: 10000  # Log training images to TensorBoard every N steps

# ──────────────────────────────────────────────────────────────────────────────
# Debug
# Options for debugging training runs.
debug:
  # Enable debug mode. This will activate additional checks and logging,
  # which may impact performance.
  enabled: false

  # If true, dump the input tensors and model outputs to a .pt file
  # when a non-finite value is detected in the model's output logits.
  # This is useful for offline analysis of the exact data causing the issue.
  dump_tensors_on_error: false

  # If true, log detailed information about the batch that caused a
  # non-finite error. This includes file paths or other identifiers from the
  # dataloader, which can help in reproducing the error.
  log_batch_info_on_error: false

  # Enable PyTorch's anomaly detection for debugging gradients.
  # This will slow down training significantly.
  detect_anomaly: false

  # If true, log the gradient norm of the model's parameters to TensorBoard.
  # This is useful for monitoring training stability.
  log_gradient_norm: false

  # If true, perform a pre-training validation step to check the integrity
  # of the input data. This can help catch issues early.
  validate_input_data: false

  # If true, save intermediate images during the data augmentation process
  # for visual inspection.
  visualize_augmentations: false

  # If true, log statistics (min/mean/max) of input batches.
  log_input_stats: false

  # If true, log statistics (min/mean/max) of model activations such as logits.
  log_activation_stats: false

  # Directory to save the visualized augmentation images.
  augmentation_visualization_path: "./aug_visualizations"
