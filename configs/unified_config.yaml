# Unified Configuration for Anime Image Tagger
# This consolidates all previously scattered configs into the Configuration_System.py structure
# Generated from: augmentation, dataloader, dataset_prep, export, inference, logging, paths, runtime, training, validation, vocabulary configs

# Global Settings
project_name: "anime-image-tagger"
experiment_name: "default_experiment"
output_root: "./experiments"
log_level: "INFO"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
# Set a target for total system RAM usage. This is a guideline and not strictly enforced.
max_memory_gb: 220.0
max_gpu_memory_gb: null

# --------------------------------------------------------------------
# Back-compat: migrated from legacy configs/paths.yaml
# These allow older scripts to read from a single unified place.
vocab_path: "./vocabulary.json"
log_dir: "./logs"
default_output_dir: "./outputs"

# Model Configuration
model:
    architecture_type: "vit_large_extended"
    hidden_size: 1536
    num_hidden_layers: 28
    num_attention_heads: 24
    intermediate_size: 6144
    image_size: 640
    patch_size: 16
    num_channels: 3
    use_cls_token: true
    use_style_token: true
    use_line_token: true
    use_color_token: true
    num_special_tokens: 4
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    drop_path_rate: 0.1   # recommended range 0.08–0.3 (must be < 1.0)
    initializer_range: 0.02
    layer_norm_eps: 1.0e-6
    use_flash_attention: true
    attention_bias: true
    token_ignore_threshold: 0.9
    num_labels: 200000
    num_groups: 20
    tags_per_group: 10000
    gradient_checkpointing: true

# Data Configuration (consolidates dataloader.yaml, train_config.yaml, augmentation.yaml)
data:
  # Storage locations
  # Add the path to your raw training data (images and JSONs) here.
  # The script will look for .json files in this directory and then find the
  # corresponding images based on the "filename" field in each JSON.
  storage_locations:
    - path: "/mnt/raid0/Danbroo/shard_00022/"  # <-- IMPORTANT: Change this to your actual data path
      priority: 0
      type: "local"
      enabled: true
    - path: "/home/user/datasets/anime_curated"
      priority: 1
      type: "local"
      enabled: false  # Disabled by default
    - path: "/mnt/das/anime_archive"
      priority: 2
      type: "das"
      enabled: false  # Disabled by default
    - path: "/mnt/nas/anime_dataset/primary"
      priority: 3
      type: "nas"
      enabled: false  # Disabled by default
    - path: "/mnt/nas/anime_dataset/video_frames"
      priority: 4
      type: "nas"
      enabled: false  # Disabled by default
  
  # Paths (from paths.yaml)
  hdf5_dir: "/home/user/datasets/teacher_features"
  vocab_dir: "./vocabulary"
  output_dir: "./outputs"
  
  # Image processing
  image_size: 640
  normalize_mean: [0.5, 0.5, 0.5]  # Anime-optimized (from inference_config.yaml)
  normalize_std: [0.5, 0.5, 0.5]   # Anime-optimized
  pad_color: [114, 114, 114]

  # Data loading (from dataloader.yaml)
  batch_size: 8  # Increased further after enabling gradient checkpointing
  num_workers: 8
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  drop_last: false
  
  # Caching
  cache_size_gb: 1024.0
  l1_per_worker_mb: 15360
  preload_files: 2
  use_memory_cache: true
  
  # L2 Caching
  l2_cache_enabled: true
  l2_cache_path: "./l2_cache"
  l2_max_size_gb: 60.0

  # Augmentation (from augmentation.yaml)
  augmentation_enabled: true
  random_flip_prob: 0.5
  color_jitter: true
  color_jitter_brightness: 0.1
  color_jitter_contrast: 0.1
  color_jitter_saturation: 0.1
  color_jitter_hue: 0.05
  eye_color_weight_boost: 0.0
  random_crop_scale: [0.8, 1.0]
  random_rotation_degrees: 0.0
  
  # Advanced Augmentation (set alpha/p to 0.0 to disable)
  randaugment_num_ops: 2
  randaugment_magnitude: 9
  mixup_alpha: 0.2
  cutmix_alpha: 1.0
  random_erasing_p: 0.5

  # Orientation mapping (from augmentation.yaml)
  orientation_map_path: "./configs/orientation_map.json"
  strict_orientation_validation: true
  skip_unmapped: true
  # Orientation flip safety policy.
  # Legacy mapping: strict → conservative, balanced → balanced, lenient → permissive
  orientation_safety_mode: "conservative"

# Training Configuration (from training_config.yaml)
training:
  num_epochs: 100
  learning_rate: 1.0e-4
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  
  # Optimizer
  optimizer: "adan"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adan_beta3: 0.99
  adam_epsilon: 1.0e-8
  
  # Scheduler
  scheduler: "cosine_restarts"
  warmup_steps: 10000
  warmup_ratio: 0.0
  num_cycles: 1.0
  lr_end: 1.0e-6
  
  # Mixed precision
  # Enabled by default to reduce VRAM usage and improve performance.
  # bfloat16 is preferred for its numerical stability and range, similar to float32.
  use_amp: true
  amp_opt_level: "O1"
  amp_dtype: bfloat16           # one of: bfloat16, float16
  enable_anomaly_detection: false  # set true for debugging only

  # Gradient clipping
  gradient_clipping:
    enabled: true
    max_norm: 1.0

  # Optionally reduce LR after a non-finite loss to help recovery.
  overflow_backoff_on_nan:
    enabled: true
    factor: 0.1   # multiply each param group lr by this factor on NaN/Inf loss

  # Checkpointing
  save_steps: 10000
  save_total_limit: 3
  save_best_only: false
  eval_steps: 1000
  logging_steps: 100
  
    # Loss configuration
    tag_loss:
      alpha: 0.5            # Balancing factor between positive/negative examples
      gamma_neg: 1.0        # Focusing parameter for negatives
      gamma_pos: 1.0        # Focusing parameter for positives
      label_smoothing: 0.0  # Smoothing for tag targets
      clip: 0.05            # Prevent extreme logits
    rating_loss:
      alpha: 0.5
      gamma_neg: 1.0
      gamma_pos: 1.0
      label_smoothing: 0.0
      clip: 0.05
    use_class_weights: true
  
  # Curriculum learning
  use_curriculum: true
  start_region_training_epoch: 20
  region_training_interval: 5
  curriculum_difficulty_schedule: "linear"
  
  # Hardware
  device: "cuda"
  distributed: false
  local_rank: -1
  world_size: 1
  ddp_backend: "nccl"
  
  # Tracking
  use_tensorboard: true
  use_wandb: false
  wandb_project: "anime-tagger"
  wandb_run_name: null
  wandb_entity: null
  
  # Training stability (from runtime.yaml)
  seed: 42
  deterministic: false
  benchmark: true
  
  # Early stopping
  early_stopping_patience: 10
  early_stopping_threshold: 0.0001

# Inference Configuration (from inference_config.yaml)
inference:
  model_path: "./checkpoints/best_model.pt"
  precision: "bf16"  # Set precision to bf16 for inference
  compile_model: false
  
  # Prediction
  prediction_threshold: 0.5
  adaptive_threshold: true
  min_predictions: 5
  max_predictions: 50
  top_k: null
  
  # Tag filtering
  filter_nsfw: false
  nsfw_tags:
    - "explicit"
    - "questionable"
    - "sensitive"
    - "nude"
    - "sexual_content"
  blacklist_tags: []
  whitelist_tags: null
  
  # Post-processing
  apply_implications: true
  resolve_aliases: true
  group_by_category: false
  remove_underscores: true
  
  # Performance
  use_tensorrt: false
  optimize_for_speed: false
  batch_timeout_ms: 100
  max_batch_size: 32
  
  # Output
  output_format: "json"
  include_scores: true
  score_decimal_places: 3
  sort_by_score: true
  
  # API Settings
  enable_api: false
  api_host: "0.0.0.0"
  api_port: 8000
  api_workers: 4
  api_max_image_size: 10485760
  api_rate_limit: 100
  api_cors_origins:
    - "*"
  
  # Caching
  enable_cache: true
  cache_ttl_seconds: 3600
  max_cache_size: 1000

# Export Configuration (from export_config.yaml)
export:
  export_format: "onnx"
  opset_version: 18
  export_params: true
  do_constant_folding: true
  dynamic_batch_size: true
  min_batch_size: 1
  max_batch_size: 128
  optimize: true
  optimize_for_mobile: false
  quantize: false
  quantization_type: "dynamic"
  calibration_dataset_size: 100
  validate_export: true
  tolerance_rtol: 1.0e-3
  tolerance_atol: 1.0e-5
  num_validation_samples: 10
  add_metadata: true
  model_description: "Anime Image Tagger Model"
  model_author: "AnimeTaggers"
  model_version: "1.0.0"
  model_license: "MIT"
  output_path: "./exported_model"

# ──────────────────────────────────────────────────────────────────────────────
# Validation (migrated from configs/validation_config.yaml)
# Centralized knobs for validation dataloader & preprocessing.
# These are read by validation_loop.py at runtime.
validation:
  dataloader:
    batch_size: 8  # Lowered to reduce VRAM usage during validation
    num_workers: 8
    prefetch_factor: 2
    persistent_workers: true
  preprocessing:
    # Uses data.normalize_mean/std by default; keep in sync with
    # training/inference unless intentionally different.
    image_size: 640
    patch_size: 16

# ──────────────────────────────────────────────────────────────────────────────
# Monitoring & Alerts
# Configures the monitoring system, including alerts for training events.
monitor:
  log_level: "INFO"
  log_to_file: true
  log_to_console: true

  # System resource tracking
  track_system_metrics: true
  system_metrics_interval: 30.0
  track_gpu_metrics: true
  track_disk_io: true
  track_network_io: false

  # Visualization
  # Note: These are often overridden by the training config for convenience
  use_tensorboard: true
  tensorboard_dir: "./tensorboard"
  use_wandb: false
  wandb_project: "anime-tagger"
  wandb_entity: null
  wandb_run_name: null

  # Alerts
  enable_alerts: true
  alert_on_gpu_memory_threshold: 0.9
  alert_on_cpu_memory_threshold: 0.9
  alert_on_disk_space_threshold: 0.95
  alert_on_training_stuck_minutes: 30
  alert_on_loss_explosion: 10.0
  alert_on_nan_loss: true
  # --------------------------------------------------------------------
  # --- WEBHOOK REVIEW POINT ---
  # The actual webhook URL is stored separately in sensitive_config.py.
  # Replace ALERT_WEBHOOK_URL in that file with your private Discord webhook.
  # alert_webhook_url: "your_webhook_url_here"
  # --------------------------------------------------------------------

  tb_image_logging:
    max_samples: 4          # how many examples to log per val step
    topk: 15                # how many tags per sample in Text tab
    log_native_resolution: true  # keep images crisp in TB
    dpi_for_figures: 220          # only used if you later add composite figures

# ──────────────────────────────────────────────────────────────────────────────
# Debug
# Options for debugging training runs.
debug:
  # Enable debug mode. This will activate additional checks and logging,
  # which may impact performance.
  enabled: false

  # If true, dump the input tensors and model outputs to a .pt file
  # when a non-finite value is detected in the model's output logits.
  # This is useful for offline analysis of the exact data causing the issue.
  dump_tensors_on_error: false

  # If true, log detailed information about the batch that caused a
  # non-finite error. This includes file paths or other identifiers from the
  # dataloader, which can help in reproducing the error.
  log_batch_info_on_error: false

  # Enable PyTorch's anomaly detection for debugging gradients.
  # This will slow down training significantly.
  detect_anomaly: false

  # If true, log the gradient norm of the model's parameters to TensorBoard.
  # This is useful for monitoring training stability.
  log_gradient_norm: false

  # If true, perform a pre-training validation step to check the integrity
  # of the input data. This can help catch issues early.
  validate_input_data: false

  # If true, save intermediate images during the data augmentation process
  # for visual inspection.
  visualize_augmentations: false

  # If true, log statistics (min/mean/max) of input batches.
  log_input_stats: false

  # If true, log statistics (min/mean/max) of model activations such as logits.
  log_activation_stats: false

  # Directory to save the visualized augmentation images.
  augmentation_visualization_path: "./aug_visualizations"
